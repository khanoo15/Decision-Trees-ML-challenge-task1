{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHI3vyhv5p85"
      },
      "source": [
        "#decision trees tuning task 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVi775ZJ2bsy"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l01c01_introduction_to_colab_and_python.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l01c01_introduction_to_colab_and_python.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rCfRNtN2XTFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8YVA_634OFk"
      },
      "source": [
        "#BaseLine DataSet\n",
        "Default Decision Tree parameters - Uses sklearn's default settings\n",
        "\n",
        "70-30 train-validation split with your ERP ID as random seed\n",
        "\n",
        "AUROC tracking on validation set\n",
        "\n",
        "Feature listing - Shows all features used and their importance"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NMP6pYsVXRCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9uIpOS2zx7k",
        "outputId": "6260838f-08e4-4215-da82-1db8a12359d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ FINAL OPTIMIZED MODEL: Backward Elimination + Best Hyperparameters\n",
            "======================================================================\n",
            "Dataset: 296209 samples, 66 features\n",
            "Target distribution: {0: 281023, 1: 15186}\n",
            "\n",
            "Baseline with best parameters: 0.6120\n",
            "=== OPTIMIZED BACKWARD ELIMINATION ===\n",
            "Initial score with all features: 0.6120\n",
            "Starting with 66 features\n",
            "Removed ps_ind_06_bin        | Features:  65 | AUROC: 0.6124 | Î”: +0.0005\n",
            "Removed feature4             | Features:  64 | AUROC: 0.6130 | Î”: +0.0006\n",
            "Removed feature2             | Features:  63 | AUROC: 0.6131 | Î”: +0.0001\n",
            "Removed id                   | Features:  62 | AUROC: 0.6131 | Î”: +0.0000\n",
            "Removed ps_ind_02_cat        | Features:  61 | AUROC: 0.6131 | Î”: +0.0000\n",
            "Removed ps_ind_04_cat        | Features:  60 | AUROC: 0.6131 | Î”: +0.0000\n",
            "Removed ps_car_01_cat        | Features:  59 | AUROC: 0.6131 | Î”: +0.0000\n",
            "Removed ps_car_02_cat        | Features:  58 | AUROC: 0.6131 | Î”: +0.0000\n",
            "Removed ps_car_03_cat        | Features:  57 | AUROC: 0.6131 | Î”: +0.0000\n",
            "Removed ps_car_05_cat        | Features:  56 | AUROC: 0.6131 | Î”: +0.0000\n",
            "Removed ps_car_06_cat        | Features:  55 | AUROC: 0.6131 | Î”: +0.0000\n",
            "Removed ps_car_08_cat        | Features:  54 | AUROC: 0.6131 | Î”: +0.0000\n",
            "Removed ps_car_09_cat        | Features:  53 | AUROC: 0.6131 | Î”: +0.0000\n",
            "Removed ps_car_10_cat        | Features:  52 | AUROC: 0.6131 | Î”: +0.0000\n",
            "Removed ps_car_11_cat        | Features:  51 | AUROC: 0.6131 | Î”: +0.0000\n",
            "Removed ps_ind_01            | Features:  50 | AUROC: 0.6131 | Î”: +0.0000\n",
            "Removed feature5             | Features:  49 | AUROC: 0.6131 | Î”: +0.0000\n",
            "Removed ps_ind_08_bin        | Features:  48 | AUROC: 0.6131 | Î”: +0.0000\n",
            "Removed ps_ind_09_bin        | Features:  47 | AUROC: 0.6131 | Î”: +0.0000\n",
            "Removed ps_ind_10_bin        | Features:  46 | AUROC: 0.6131 | Î”: +0.0000\n",
            "Removed ps_ind_11_bin        | Features:  45 | AUROC: 0.6131 | Î”: +0.0000\n",
            "Removed ps_ind_12_bin        | Features:  44 | AUROC: 0.6131 | Î”: +0.0000\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set random seed\n",
        "ERP_ID = 29077\n",
        "np.random.seed(ERP_ID)\n",
        "\n",
        "print(\"=== DECISION TREE BASELINE MODEL ===\")\n",
        "\n",
        "# Load the dataset\n",
        "def load_data():\n",
        "    try:\n",
        "        df = pd.read_csv('train1.csv')\n",
        "        print(f\"Dataset loaded with shape: {df.shape}\")\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: train1.csv not found\")\n",
        "        return None\n",
        "\n",
        "# Data exploration\n",
        "def explore_data(df):\n",
        "    print(\"\\n=== DATA EXPLORATION ===\")\n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "    print(f\"Columns: {df.columns.tolist()}\")\n",
        "\n",
        "    # Check data types\n",
        "    print(f\"\\nData types:\")\n",
        "    print(df.dtypes.value_counts())\n",
        "\n",
        "    # Check target variable\n",
        "    if 'target' in df.columns:\n",
        "        target_dist = df['target'].value_counts()\n",
        "        print(f\"\\nTarget distribution:\")\n",
        "        print(target_dist)\n",
        "        print(f\"Target proportion:\")\n",
        "        print(df['target'].value_counts(normalize=True))\n",
        "\n",
        "    # Check for missing values\n",
        "    missing_values = df.isnull().sum()\n",
        "    if missing_values.sum() > 0:\n",
        "        print(f\"\\nMissing values found:\")\n",
        "        print(missing_values[missing_values > 0])\n",
        "    else:\n",
        "        print(f\"\\nNo missing values found\")\n",
        "\n",
        "# Preprocessing for baseline\n",
        "def preprocess_baseline(df):\n",
        "    df_processed = df.copy()\n",
        "\n",
        "    # Separate features and target\n",
        "    X = df_processed.drop('target', axis=1)\n",
        "    y = df_processed['target']\n",
        "\n",
        "    # Handle missing values - simple mean imputation for numeric\n",
        "    for col in X.columns:\n",
        "        if X[col].dtype in ['int64', 'float64']:\n",
        "            X[col] = X[col].fillna(X[col].mean())\n",
        "        else:\n",
        "            # For categorical columns, use mode\n",
        "            X[col] = X[col].fillna(X[col].mode()[0] if len(X[col].mode()) > 0 else 'Unknown')\n",
        "\n",
        "    # Convert categorical variables to numeric using simple label encoding\n",
        "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "    if len(categorical_cols) > 0:\n",
        "        for col in categorical_cols:\n",
        "            X[col] = X[col].astype('category').cat.codes\n",
        "        print(f\"Categorical columns processed: {len(categorical_cols)}\")\n",
        "    else:\n",
        "        print(f\"No categorical columns found\")\n",
        "        categorical_cols = []\n",
        "\n",
        "    print(f\"Preprocessing completed\")\n",
        "    print(f\"Features shape: {X.shape}\")\n",
        "\n",
        "    return X, y, categorical_cols.tolist()\n",
        "\n",
        "# Create baseline model\n",
        "def create_baseline_model(X, y):\n",
        "    print(\"\\n=== CREATING BASELINE MODEL ===\")\n",
        "\n",
        "    # Train-validation split (70-30)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=ERP_ID, stratify=y\n",
        "    )\n",
        "\n",
        "    print(f\"Training set size: {X_train.shape[0]} samples\")\n",
        "    print(f\"Validation set size: {X_val.shape[0]} samples\")\n",
        "    print(f\"Number of features: {X_train.shape[1]}\")\n",
        "\n",
        "    # Baseline Decision Tree with default parameters\n",
        "    baseline_dt = DecisionTreeClassifier(random_state=ERP_ID)\n",
        "\n",
        "    print(f\"\\nTraining baseline model with DEFAULT parameters:\")\n",
        "    print(f\"{baseline_dt.get_params()}\")\n",
        "\n",
        "    # Train the model\n",
        "    baseline_dt.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions on validation set\n",
        "    y_val_pred_proba = baseline_dt.predict_proba(X_val)[:, 1]\n",
        "    y_val_pred = baseline_dt.predict(X_val)\n",
        "\n",
        "    # Calculate AUROC\n",
        "    baseline_auroc = roc_auc_score(y_val, y_val_pred_proba)\n",
        "\n",
        "    print(f\"\\nBASELINE MODEL PERFORMANCE:\")\n",
        "    print(f\"AUROC on Validation Set: {baseline_auroc:.4f}\")\n",
        "\n",
        "    # Additional metrics\n",
        "    print(f\"\\nClassification Report:\")\n",
        "    print(classification_report(y_val, y_val_pred))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_val, y_val_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title('Baseline Model - Confusion Matrix')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.show()\n",
        "\n",
        "    return X_train, X_val, y_train, y_val, baseline_dt, baseline_auroc\n",
        "\n",
        "# Feature analysis\n",
        "def analyze_features(baseline_dt, feature_names):\n",
        "    print(\"\\n=== FEATURE ANALYSIS ===\")\n",
        "\n",
        "    # Get feature importance\n",
        "    feature_importance = baseline_dt.feature_importances_\n",
        "\n",
        "    # Create feature importance dataframe\n",
        "    feature_imp_df = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': feature_importance\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    print(\"Top 20 Most Important Features:\")\n",
        "    print(feature_imp_df.head(20))\n",
        "\n",
        "    # Plot top 15 features\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    top_features = feature_imp_df.head(15)\n",
        "    sns.barplot(data=top_features, x='importance', y='feature')\n",
        "    plt.title('Top 15 Feature Importance - Baseline Model')\n",
        "    plt.xlabel('Feature Importance')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # List all features used\n",
        "    print(f\"\\nALL FEATURES USED IN BASELINE MODEL ({len(feature_names)} features):\")\n",
        "    for i, feature in enumerate(feature_names, 1):\n",
        "        print(f\"{i:2d}. {feature}\")\n",
        "\n",
        "    return feature_imp_df\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    print(f\"STARTING BASELINE DECISION TREE ANALYSIS\")\n",
        "    print(f\"ERP ID: {ERP_ID}\")\n",
        "    print(f\"Random Seed: {ERP_ID}\")\n",
        "\n",
        "    # Load data\n",
        "    df = load_data()\n",
        "    if df is None:\n",
        "        return\n",
        "\n",
        "    # Explore data\n",
        "    explore_data(df)\n",
        "\n",
        "    # Preprocess data\n",
        "    X, y, categorical_cols = preprocess_baseline(df)\n",
        "\n",
        "    # Create baseline model\n",
        "    X_train, X_val, y_train, y_val, baseline_dt, baseline_auroc = create_baseline_model(X, y)\n",
        "\n",
        "    # Analyze features\n",
        "    feature_imp_df = analyze_features(baseline_dt, X.columns.tolist())\n",
        "\n",
        "    # Final summary\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"BASELINE MODEL SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Dataset: train1.csv\")\n",
        "    print(f\"Train-Validation Split: 70%-30%\")\n",
        "    print(f\"Random State: {ERP_ID}\")\n",
        "    print(f\"Model: Decision Tree (Default Parameters)\")\n",
        "    print(f\"Validation AUROC: {baseline_auroc:.4f}\")\n",
        "    print(f\"Features Used: {X.shape[1]}\")\n",
        "    print(f\"Training Samples: {X_train.shape[0]}\")\n",
        "    print(f\"Validation Samples: {X_val.shape[0]}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Save baseline results for comparison in next steps\n",
        "    baseline_results = {\n",
        "        'model': baseline_dt,\n",
        "        'auroc': baseline_auroc,\n",
        "        'X_train': X_train,\n",
        "        'X_val': X_val,\n",
        "        'y_train': y_train,\n",
        "        'y_val': y_val,\n",
        "        'feature_importance': feature_imp_df\n",
        "    }\n",
        "\n",
        "    return baseline_results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    baseline_results = main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwJGmDrQ0EoB"
      },
      "source": [
        "##Feature Selection Implementation\n",
        "Test each technique SEPARATATELY and track AUROC at each stage:\n",
        "Baseline (default model)\n",
        "After feature selection (ANOVA + Chi-Square)\n",
        "After wrapper methods (Forward/Backward)\n",
        "Compare results and see which individual technique gives the best improvement\n",
        "Choose the best performing approach for the final model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pRllo2HLfXiu",
        "outputId": "6e2da239-ece6-469f-8129-430886395443",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Overview:\n",
            "Samples: 296209, Features: 66\n",
            "Numeric: 66, Categorical: 0\n",
            "\n",
            "=== BASELINE PERFORMANCE ===\n",
            "Baseline AUROC: 0.5062\n",
            "Number of features: 66\n",
            "\n",
            "=== ANOVA + CHI-SQUARE FEATURE SELECTION ===\n",
            "ANOVA selected 33 numeric features\n",
            "AUROC after ANOVA+Chi2: 0.5096\n",
            "Features selected: 33\n",
            "\n",
            "=== FORWARD SELECTION ===\n",
            "Step 1: Added ps_car_13, AUROC: 0.5751\n",
            "Step 2: Added ps_ind_05_cat, AUROC: 0.5900\n",
            "Step 3: Added ps_ind_17_bin, AUROC: 0.6006\n",
            "Step 4: Added ps_ind_06_bin, AUROC: 0.6037\n",
            "Step 5: Added feature4, AUROC: 0.6056\n",
            "Step 6: Added ps_car_03_cat, AUROC: 0.6068\n",
            "Step 7: Added feature2, AUROC: 0.6072\n",
            "Step 8: Added ps_ind_15, AUROC: 0.6074\n",
            "Step 9: Added ps_car_09_cat, AUROC: 0.6075\n",
            "Step 10: Added ps_car_12, AUROC: 0.6076\n",
            "Final AUROC after forward selection: 0.5066\n",
            "Features selected: 10\n",
            "\n",
            "=== BACKWARD ELIMINATION ===\n",
            "Initial AUROC with all features: 0.6053\n",
            "Removed ps_ind_06_bin, AUROC: 0.6065, Features left: 65\n",
            "Removed ps_reg_03, AUROC: 0.6071, Features left: 64\n",
            "Removed feature4, AUROC: 0.6072, Features left: 63\n",
            "Removed ps_ind_17_bin, AUROC: 0.6074, Features left: 62\n",
            "Removed ps_car_14, AUROC: 0.6075, Features left: 61\n",
            "Removed feature7, AUROC: 0.6076, Features left: 60\n",
            "Removed feature2, AUROC: 0.6077, Features left: 59\n",
            "Removed ps_car_12, AUROC: 0.6077, Features left: 58\n",
            "Removed ps_calc_07, AUROC: 0.6077, Features left: 57\n",
            "Removed feature3, AUROC: 0.6077, Features left: 56\n",
            "Removed ps_ind_02_cat, AUROC: 0.6077, Features left: 55\n",
            "Final AUROC after backward elimination: 0.5109\n",
            "Features remaining: 55\n",
            "\n",
            "=== RESULTS SUMMARY ===\n",
            "Method                 | AUROC    | Improvement\n",
            "----------------------------------------\n",
            "Baseline               | 0.5062  | +0.0000\n",
            "ANOVA+Chi2             | 0.5096  | +0.0034\n",
            "Forward Selection      | 0.5066  | +0.0005\n",
            "Backward Elimination   | 0.5109  | +0.0047\n",
            "\n",
            "Best method: Backward Elimination (AUROC: 0.5109)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "ERP_ID = 29077\n",
        "np.random.seed(ERP_ID)\n",
        "\n",
        "def load_and_preprocess_data():\n",
        "    df = pd.read_csv('train1.csv')\n",
        "    X = df.drop('target', axis=1)\n",
        "    y = df['target']\n",
        "\n",
        "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "    for col in categorical_cols:\n",
        "        X[col] = LabelEncoder().fit_transform(X[col].astype(str))\n",
        "\n",
        "    for col in numeric_cols:\n",
        "        X[col] = X[col].fillna(X[col].mean())\n",
        "\n",
        "    return X, y, categorical_cols.tolist(), numeric_cols.tolist()\n",
        "\n",
        "def evaluate_model(X, y, feature_set_name, features):\n",
        "    X_selected = X[features]\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_selected, y, test_size=0.3, random_state=ERP_ID, stratify=y\n",
        "    )\n",
        "\n",
        "    dt = DecisionTreeClassifier(random_state=ERP_ID)\n",
        "    dt.fit(X_train, y_train)\n",
        "\n",
        "    y_pred_proba = dt.predict_proba(X_val)[:, 1]\n",
        "    auroc = roc_auc_score(y_val, y_pred_proba)\n",
        "\n",
        "    return auroc\n",
        "\n",
        "def baseline_performance(X, y):\n",
        "    print(\"=== BASELINE PERFORMANCE ===\")\n",
        "    baseline_auc = evaluate_model(X, y, \"Baseline\", X.columns.tolist())\n",
        "    print(f\"Baseline AUROC: {baseline_auc:.4f}\")\n",
        "    print(f\"Number of features: {X.shape[1]}\")\n",
        "    print()\n",
        "    return baseline_auc\n",
        "\n",
        "def anova_chi2_feature_selection(X, y, numeric_cols, categorical_cols):\n",
        "    print(\"=== ANOVA + CHI-SQUARE FEATURE SELECTION ===\")\n",
        "\n",
        "    selected_features = []\n",
        "\n",
        "    # ANOVA for numeric features\n",
        "    if numeric_cols:\n",
        "        X_numeric = X[numeric_cols]\n",
        "        anova_selector = SelectKBest(score_func=f_classif, k='all')\n",
        "        anova_selector.fit(X_numeric, y)\n",
        "        numeric_scores = anova_selector.scores_\n",
        "\n",
        "        # Select top 50% numeric features\n",
        "        k_numeric = max(1, len(numeric_cols) // 2)\n",
        "        top_numeric_indices = np.argsort(numeric_scores)[-k_numeric:]\n",
        "        numeric_selected = [numeric_cols[i] for i in top_numeric_indices]\n",
        "        selected_features.extend(numeric_selected)\n",
        "        print(f\"ANOVA selected {len(numeric_selected)} numeric features\")\n",
        "\n",
        "    # Chi-Square for categorical features\n",
        "    if categorical_cols:\n",
        "        X_categorical = X[categorical_cols]\n",
        "        chi2_selector = SelectKBest(score_func=chi2, k='all')\n",
        "        chi2_selector.fit(X_categorical, y)\n",
        "        categorical_scores = chi2_selector.scores_\n",
        "\n",
        "        # Select top 50% categorical features\n",
        "        k_categorical = max(1, len(categorical_cols) // 2)\n",
        "        top_categorical_indices = np.argsort(categorical_scores)[-k_categorical:]\n",
        "        categorical_selected = [categorical_cols[i] for i in top_categorical_indices]\n",
        "        selected_features.extend(categorical_selected)\n",
        "        print(f\"Chi-Square selected {len(categorical_selected)} categorical features\")\n",
        "\n",
        "    auroc = evaluate_model(X, y, \"ANOVA+Chi2\", selected_features)\n",
        "    print(f\"AUROC after ANOVA+Chi2: {auroc:.4f}\")\n",
        "    print(f\"Features selected: {len(selected_features)}\")\n",
        "    print()\n",
        "\n",
        "    return auroc, selected_features\n",
        "\n",
        "def forward_selection(X, y, max_features=15):\n",
        "    print(\"=== FORWARD SELECTION ===\")\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=ERP_ID, stratify=y\n",
        "    )\n",
        "\n",
        "    remaining_features = X.columns.tolist()\n",
        "    selected_features = []\n",
        "    best_score = 0\n",
        "\n",
        "    for i in range(min(max_features, len(remaining_features))):\n",
        "        scores = []\n",
        "\n",
        "        for feature in remaining_features:\n",
        "            if feature not in selected_features:\n",
        "                current_features = selected_features + [feature]\n",
        "                dt = DecisionTreeClassifier(random_state=ERP_ID, max_depth=5)\n",
        "                dt.fit(X_train[current_features], y_train)\n",
        "                y_pred_proba = dt.predict_proba(X_val[current_features])[:, 1]\n",
        "                score = roc_auc_score(y_val, y_pred_proba)\n",
        "                scores.append((score, feature))\n",
        "\n",
        "        if not scores:\n",
        "            break\n",
        "\n",
        "        best_current_score, best_feature = max(scores, key=lambda x: x[0])\n",
        "\n",
        "        if best_current_score > best_score:\n",
        "            selected_features.append(best_feature)\n",
        "            best_score = best_current_score\n",
        "            print(f\"Step {i+1}: Added {best_feature}, AUROC: {best_current_score:.4f}\")\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    # Final evaluation with selected features\n",
        "    final_auc = evaluate_model(X, y, \"Forward Selection\", selected_features)\n",
        "    print(f\"Final AUROC after forward selection: {final_auc:.4f}\")\n",
        "    print(f\"Features selected: {len(selected_features)}\")\n",
        "    print()\n",
        "\n",
        "    return final_auc, selected_features\n",
        "\n",
        "def backward_elimination(X, y, min_features=5):\n",
        "    print(\"=== BACKWARD ELIMINATION ===\")\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=ERP_ID, stratify=y\n",
        "    )\n",
        "\n",
        "    current_features = X.columns.tolist()\n",
        "    dt_full = DecisionTreeClassifier(random_state=ERP_ID, max_depth=5)\n",
        "    dt_full.fit(X_train[current_features], y_train)\n",
        "    y_pred_proba = dt_full.predict_proba(X_val[current_features])[:, 1]\n",
        "    current_score = roc_auc_score(y_val, y_pred_proba)\n",
        "\n",
        "    print(f\"Initial AUROC with all features: {current_score:.4f}\")\n",
        "\n",
        "    while len(current_features) > min_features:\n",
        "        scores = []\n",
        "\n",
        "        for feature in current_features:\n",
        "            features_without = [f for f in current_features if f != feature]\n",
        "            dt = DecisionTreeClassifier(random_state=ERP_ID, max_depth=5)\n",
        "            dt.fit(X_train[features_without], y_train)\n",
        "            y_pred_proba = dt.predict_proba(X_val[features_without])[:, 1]\n",
        "            score = roc_auc_score(y_val, y_pred_proba)\n",
        "            scores.append((score, feature))\n",
        "\n",
        "        best_score, worst_feature = max(scores, key=lambda x: x[0])\n",
        "\n",
        "        if best_score >= current_score:\n",
        "            current_features.remove(worst_feature)\n",
        "            current_score = best_score\n",
        "            print(f\"Removed {worst_feature}, AUROC: {current_score:.4f}, Features left: {len(current_features)}\")\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    # Final evaluation\n",
        "    final_auc = evaluate_model(X, y, \"Backward Elimination\", current_features)\n",
        "    print(f\"Final AUROC after backward elimination: {final_auc:.4f}\")\n",
        "    print(f\"Features remaining: {len(current_features)}\")\n",
        "    print()\n",
        "\n",
        "    return final_auc, current_features\n",
        "\n",
        "def main():\n",
        "    X, y, categorical_cols, numeric_cols = load_and_preprocess_data()\n",
        "\n",
        "    print(\"Dataset Overview:\")\n",
        "    print(f\"Samples: {X.shape[0]}, Features: {X.shape[1]}\")\n",
        "    print(f\"Numeric: {len(numeric_cols)}, Categorical: {len(categorical_cols)}\")\n",
        "    print()\n",
        "\n",
        "    # Track results\n",
        "    results = {}\n",
        "\n",
        "    # 1. Baseline\n",
        "    baseline_auc = baseline_performance(X, y)\n",
        "    results['Baseline'] = baseline_auc\n",
        "\n",
        "    # 2. ANOVA + Chi-Square\n",
        "    anova_auc, anova_features = anova_chi2_feature_selection(X, y, numeric_cols, categorical_cols)\n",
        "    results['ANOVA+Chi2'] = anova_auc\n",
        "\n",
        "    # 3. Forward Selection\n",
        "    forward_auc, forward_features = forward_selection(X, y)\n",
        "    results['Forward Selection'] = forward_auc\n",
        "\n",
        "    # 4. Backward Elimination\n",
        "    backward_auc, backward_features = backward_elimination(X, y)\n",
        "    results['Backward Elimination'] = backward_auc\n",
        "\n",
        "    # Summary\n",
        "    print(\"=== RESULTS SUMMARY ===\")\n",
        "    print(\"Method                 | AUROC    | Improvement\")\n",
        "    print(\"-\" * 40)\n",
        "    for method, auroc in results.items():\n",
        "        improvement = auroc - baseline_auc\n",
        "        print(f\"{method:22} | {auroc:.4f}  | {improvement:+.4f}\")\n",
        "\n",
        "    best_method = max(results.items(), key=lambda x: x[1])\n",
        "    print(f\"\\nBest method: {best_method[0]} (AUROC: {best_method[1]:.4f})\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiZG7uhm8qCF"
      },
      "source": [
        "Easy, right?\n",
        "\n",
        "If you want a loop starting at 0 to 2 (exclusive) you could do any of the following"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "m8YQN1H41L-Y",
        "outputId": "36125329-2273-4ce9-8537-a0fb3db203fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Set 1: AUROC = 0.5792 | {'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 5, 'class_weight': 'balanced'}\n",
            "Set 2: AUROC = 0.5342 | {'max_depth': 15, 'min_samples_split': 5, 'min_samples_leaf': 2, 'class_weight': 'balanced'}\n",
            "Set 3: AUROC = 0.5083 | {'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1, 'class_weight': None}\n",
            "Set 4: AUROC = 0.5962 | {'max_depth': 8, 'min_samples_split': 15, 'min_samples_leaf': 8, 'class_weight': 'balanced'}\n",
            "\n",
            "Best: AUROC = 0.5962\n",
            "Best params: {'max_depth': 8, 'min_samples_split': 15, 'min_samples_leaf': 8, 'class_weight': 'balanced'}\n",
            "\n",
            "Top 10 features:\n",
            "          feature  importance\n",
            "35      ps_car_13    0.237593\n",
            "3   ps_ind_05_cat    0.109205\n",
            "32      ps_reg_03    0.098694\n",
            "28  ps_ind_17_bin    0.048376\n",
            "26      ps_ind_15    0.046693\n",
            "61       feature4    0.040825\n",
            "63       feature6    0.035911\n",
            "16      ps_ind_03    0.032635\n",
            "59       feature2    0.027755\n",
            "0              id    0.022982\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(np.float64(0.5962162157926245),\n",
              " {'max_depth': 8,\n",
              "  'min_samples_split': 15,\n",
              "  'min_samples_leaf': 8,\n",
              "  'class_weight': 'balanced'})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "ERP_ID = 29077\n",
        "np.random.seed(ERP_ID)\n",
        "\n",
        "def ultra_fast_tuning():\n",
        "    # Load data\n",
        "    df = pd.read_csv('train1.csv')\n",
        "    X = df.drop('target', axis=1)\n",
        "    y = df['target']\n",
        "\n",
        "    # Quick preprocessing\n",
        "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "    for col in categorical_cols:\n",
        "        X[col] = LabelEncoder().fit_transform(X[col].astype(str))\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=ERP_ID, stratify=y\n",
        "    )\n",
        "\n",
        "    # Test only 4 key parameter combinations\n",
        "    param_sets = [\n",
        "        {'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 5, 'class_weight': 'balanced'},\n",
        "        {'max_depth': 15, 'min_samples_split': 5, 'min_samples_leaf': 2, 'class_weight': 'balanced'},\n",
        "        {'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1, 'class_weight': None},\n",
        "        {'max_depth': 8, 'min_samples_split': 15, 'min_samples_leaf': 8, 'class_weight': 'balanced'}\n",
        "    ]\n",
        "\n",
        "    best_auc = 0\n",
        "    best_params = {}\n",
        "\n",
        "    for i, params in enumerate(param_sets, 1):\n",
        "        dt = DecisionTreeClassifier(**params, random_state=ERP_ID)\n",
        "        dt.fit(X_train, y_train)\n",
        "        y_pred = dt.predict_proba(X_val)[:, 1]\n",
        "        auc = roc_auc_score(y_val, y_pred)\n",
        "\n",
        "        print(f\"Set {i}: AUROC = {auc:.4f} | {params}\")\n",
        "\n",
        "        if auc > best_auc:\n",
        "            best_auc = auc\n",
        "            best_params = params\n",
        "\n",
        "    print(f\"\\nBest: AUROC = {best_auc:.4f}\")\n",
        "    print(f\"Best params: {best_params}\")\n",
        "\n",
        "    # Feature importance\n",
        "    best_dt = DecisionTreeClassifier(**best_params, random_state=ERP_ID)\n",
        "    best_dt.fit(X_train, y_train)\n",
        "    importances = best_dt.feature_importances_\n",
        "\n",
        "    feature_imp = pd.DataFrame({\n",
        "        'feature': X.columns,\n",
        "        'importance': importances\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    print(\"\\nTop 10 features:\")\n",
        "    print(feature_imp.head(10))\n",
        "\n",
        "    return best_auc, best_params\n",
        "\n",
        "# Run it\n",
        "ultra_fast_tuning()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QyOUhFw1OUX"
      },
      "source": [
        "#PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4Dxk4q-jzEy4",
        "outputId": "469223a3-0145-4621-bf6c-a55d5f5396d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Overview:\n",
            "Samples: 296209, Features: 66\n",
            "\n",
            "Backward elimination selected 55 features\n",
            "=== HYPERPARAMETER TUNING ON BACKWARD ELIMINATION FEATURES ===\n",
            "Best parameters: {'min_samples_split': 5, 'min_samples_leaf': 5, 'max_depth': 5, 'criterion': 'entropy', 'class_weight': None}\n",
            "Tuned AUROC: 0.6098\n",
            "Number of features: 55\n",
            "\n",
            "=== COMPARISON ===\n",
            "Baseline AUROC (with backward features): 0.5109\n",
            "Tuned AUROC: 0.6098\n",
            "Improvement: +0.0989\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "ERP_ID = 29077\n",
        "np.random.seed(ERP_ID)\n",
        "\n",
        "def load_and_preprocess_data():\n",
        "    df = pd.read_csv('train1.csv')\n",
        "    X = df.drop('target', axis=1)\n",
        "    y = df['target']\n",
        "\n",
        "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "    for col in categorical_cols:\n",
        "        X[col] = LabelEncoder().fit_transform(X[col].astype(str))\n",
        "\n",
        "    for col in numeric_cols:\n",
        "        X[col] = X[col].fillna(X[col].mean())\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def backward_elimination(X, y, min_features=5):\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=ERP_ID, stratify=y\n",
        "    )\n",
        "\n",
        "    current_features = X.columns.tolist()\n",
        "    dt_full = DecisionTreeClassifier(random_state=ERP_ID, max_depth=5)\n",
        "    dt_full.fit(X_train[current_features], y_train)\n",
        "    y_pred_proba = dt_full.predict_proba(X_val[current_features])[:, 1]\n",
        "    current_score = roc_auc_score(y_val, y_pred_proba)\n",
        "\n",
        "    while len(current_features) > min_features:\n",
        "        scores = []\n",
        "        for feature in current_features:\n",
        "            features_without = [f for f in current_features if f != feature]\n",
        "            dt = DecisionTreeClassifier(random_state=ERP_ID, max_depth=5)\n",
        "            dt.fit(X_train[features_without], y_train)\n",
        "            y_pred_proba = dt.predict_proba(X_val[features_without])[:, 1]\n",
        "            score = roc_auc_score(y_val, y_pred_proba)\n",
        "            scores.append((score, feature))\n",
        "\n",
        "        best_score, worst_feature = max(scores, key=lambda x: x[0])\n",
        "        if best_score >= current_score:\n",
        "            current_features.remove(worst_feature)\n",
        "            current_score = best_score\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return current_features\n",
        "\n",
        "def hyperparameter_tuning(X, y, selected_features):\n",
        "    print(\"=== HYPERPARAMETER TUNING ON BACKWARD ELIMINATION FEATURES ===\")\n",
        "\n",
        "    X_selected = X[selected_features]\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_selected, y, test_size=0.3, random_state=ERP_ID, stratify=y\n",
        "    )\n",
        "\n",
        "    # Parameter grid for tuning\n",
        "    param_grid = {\n",
        "        'max_depth': [3, 5, 7, 10, 15, 20, None],\n",
        "        'min_samples_split': [2, 5, 10, 20],\n",
        "        'min_samples_leaf': [1, 2, 5, 10],\n",
        "        'criterion': ['gini', 'entropy'],\n",
        "        'class_weight': [None, 'balanced']\n",
        "    }\n",
        "\n",
        "    # Use RandomizedSearchCV for efficiency\n",
        "    dt = DecisionTreeClassifier(random_state=ERP_ID)\n",
        "    random_search = RandomizedSearchCV(\n",
        "        dt, param_grid, n_iter=50, cv=3, scoring='roc_auc',\n",
        "        random_state=ERP_ID, n_jobs=-1\n",
        "    )\n",
        "\n",
        "    random_search.fit(X_train, y_train)\n",
        "\n",
        "    # Best model evaluation\n",
        "    best_dt = random_search.best_estimator_\n",
        "    y_pred_proba = best_dt.predict_proba(X_val)[:, 1]\n",
        "    tuned_auc = roc_auc_score(y_val, y_pred_proba)\n",
        "\n",
        "    print(f\"Best parameters: {random_search.best_params_}\")\n",
        "    print(f\"Tuned AUROC: {tuned_auc:.4f}\")\n",
        "    print(f\"Number of features: {len(selected_features)}\")\n",
        "\n",
        "    return tuned_auc, best_dt, random_search.best_params_\n",
        "\n",
        "def main():\n",
        "    X, y = load_and_preprocess_data()\n",
        "\n",
        "    print(\"Dataset Overview:\")\n",
        "    print(f\"Samples: {X.shape[0]}, Features: {X.shape[1]}\")\n",
        "    print()\n",
        "\n",
        "    # Get backward elimination features\n",
        "    backward_features = backward_elimination(X, y)\n",
        "    print(f\"Backward elimination selected {len(backward_features)} features\")\n",
        "\n",
        "    # Hyperparameter tuning\n",
        "    tuned_auc, best_model, best_params = hyperparameter_tuning(X, y, backward_features)\n",
        "\n",
        "    # Compare with baseline\n",
        "    X_baseline = X[backward_features]\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_baseline, y, test_size=0.3, random_state=ERP_ID, stratify=y\n",
        "    )\n",
        "\n",
        "    dt_baseline = DecisionTreeClassifier(random_state=ERP_ID)\n",
        "    dt_baseline.fit(X_train, y_train)\n",
        "    baseline_pred_proba = dt_baseline.predict_proba(X_val)[:, 1]\n",
        "    baseline_auc = roc_auc_score(y_val, baseline_pred_proba)\n",
        "\n",
        "    print(f\"\\n=== COMPARISON ===\")\n",
        "    print(f\"Baseline AUROC (with backward features): {baseline_auc:.4f}\")\n",
        "    print(f\"Tuned AUROC: {tuned_auc:.4f}\")\n",
        "    print(f\"Improvement: {tuned_auc - baseline_auc:+.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Final code"
      ],
      "metadata": {
        "id": "NcZ7IN4cXoSx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTa8_9G3LV03"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "ERP_ID = 29077\n",
        "np.random.seed(ERP_ID)\n",
        "\n",
        "def backward_elimination_optimized(X, y, best_params):\n",
        "    \"\"\"Optimized backward elimination using best hyperparameters\"\"\"\n",
        "    print(\"=== OPTIMIZED BACKWARD ELIMINATION ===\")\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=ERP_ID, stratify=y\n",
        "    )\n",
        "\n",
        "    current_features = X.columns.tolist()\n",
        "\n",
        "    # Start with best model on all features\n",
        "    dt_best = DecisionTreeClassifier(**best_params, random_state=ERP_ID)\n",
        "    dt_best.fit(X_train[current_features], y_train)\n",
        "    y_pred_proba = dt_best.predict_proba(X_val[current_features])[:, 1]\n",
        "    current_score = roc_auc_score(y_val, y_pred_proba)\n",
        "\n",
        "    print(f\"Initial score with all features: {current_score:.4f}\")\n",
        "    print(f\"Starting with {len(current_features)} features\")\n",
        "\n",
        "    removed_features = []\n",
        "    improvement_history = []\n",
        "\n",
        "    # Backward elimination\n",
        "    while len(current_features) > 30:  # Stop at reasonable number\n",
        "        scores = []\n",
        "\n",
        "        for feature in current_features:\n",
        "            features_without = [f for f in current_features if f != feature]\n",
        "            dt = DecisionTreeClassifier(**best_params, random_state=ERP_ID)\n",
        "            dt.fit(X_train[features_without], y_train)\n",
        "            y_pred_proba = dt.predict_proba(X_val[features_without])[:, 1]\n",
        "            score = roc_auc_score(y_val, y_pred_proba)\n",
        "            scores.append((score, feature))\n",
        "\n",
        "        best_score, worst_feature = max(scores, key=lambda x: x[0])\n",
        "\n",
        "        if best_score >= current_score - 0.001:  # Small tolerance for minor drops\n",
        "            current_features.remove(worst_feature)\n",
        "            removed_features.append(worst_feature)\n",
        "            improvement = best_score - current_score\n",
        "            improvement_history.append(improvement)\n",
        "            current_score = best_score\n",
        "            print(f\"Removed {worst_feature:20} | Features: {len(current_features):3d} | AUROC: {current_score:.4f} | Î”: {improvement:+.4f}\")\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    print(f\"\\nBackward elimination completed!\")\n",
        "    print(f\"Final features: {len(current_features)}\")\n",
        "    print(f\"Final AUROC: {current_score:.4f}\")\n",
        "\n",
        "    return current_features, current_score, removed_features\n",
        "\n",
        "def final_tuned_model(X, y, selected_features, best_params):\n",
        "    \"\"\"Train final model with selected features and best parameters\"\"\"\n",
        "    print(\"\\n=== FINAL TUNED MODEL ===\")\n",
        "\n",
        "    X_selected = X[selected_features]\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_selected, y, test_size=0.3, random_state=ERP_ID, stratify=y\n",
        "    )\n",
        "\n",
        "    # Train with best parameters on selected features\n",
        "    final_dt = DecisionTreeClassifier(**best_params, random_state=ERP_ID)\n",
        "    final_dt.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_proba = final_dt.predict_proba(X_val)[:, 1]\n",
        "    final_auc = roc_auc_score(y_val, y_pred_proba)\n",
        "\n",
        "    print(f\"Final Model AUROC: {final_auc:.4f}\")\n",
        "    print(f\"Features used: {len(selected_features)}\")\n",
        "    print(f\"Best parameters: {best_params}\")\n",
        "\n",
        "    return final_dt, final_auc\n",
        "\n",
        "def feature_importance_analysis(model, feature_names):\n",
        "    \"\"\"Comprehensive feature importance analysis\"\"\"\n",
        "    print(\"\\n=== FEATURE IMPORTANCE ANALYSIS ===\")\n",
        "\n",
        "    importances = model.feature_importances_\n",
        "    feature_imp_df = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': importances\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    print(\"Top 15 Most Important Features:\")\n",
        "    print(\"-\" * 50)\n",
        "    for i, row in feature_imp_df.head(15).iterrows():\n",
        "        print(f\"{i+1:2d}. {row['feature']:30} : {row['importance']:.4f} ({row['importance']*100:.1f}%)\")\n",
        "\n",
        "    # Cumulative importance\n",
        "    cumulative = np.cumsum(feature_imp_df['importance'])\n",
        "    n_80 = np.argmax(cumulative >= 0.8) + 1\n",
        "    n_90 = np.argmax(cumulative >= 0.9) + 1\n",
        "\n",
        "    print(f\"\\nCumulative Importance Analysis:\")\n",
        "    print(f\"Top {n_80} features explain 80% of importance\")\n",
        "    print(f\"Top {n_90} features explain 90% of importance\")\n",
        "    print(f\"Top 10 features explain {cumulative[9]:.1%} of importance\")\n",
        "\n",
        "    # Visualization\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    top_features = feature_imp_df.head(15)\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.barh(range(len(top_features)), top_features['importance'])\n",
        "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
        "    plt.xlabel('Feature Importance')\n",
        "    plt.title('Top 15 Feature Importance')\n",
        "    plt.gca().invert_yaxis()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(range(1, len(cumulative) + 1), cumulative, 'b-', linewidth=2)\n",
        "    plt.axhline(y=0.8, color='r', linestyle='--', label='80%')\n",
        "    plt.axhline(y=0.9, color='g', linestyle='--', label='90%')\n",
        "    plt.xlabel('Number of Features')\n",
        "    plt.ylabel('Cumulative Importance')\n",
        "    plt.title('Cumulative Feature Importance')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('final_feature_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    return feature_imp_df\n",
        "\n",
        "def main():\n",
        "    print(\"ðŸš€ FINAL OPTIMIZED MODEL: Backward Elimination + Best Hyperparameters\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Load data\n",
        "    df = pd.read_csv('train1.csv')\n",
        "    X = df.drop('target', axis=1)\n",
        "    y = df['target']\n",
        "\n",
        "    # Preprocessing\n",
        "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "    for col in categorical_cols:\n",
        "        X[col] = LabelEncoder().fit_transform(X[col].astype(str))\n",
        "\n",
        "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
        "    for col in numeric_cols:\n",
        "        X[col] = X[col].fillna(X[col].mean())\n",
        "\n",
        "    print(f\"Dataset: {X.shape[0]} samples, {X.shape[1]} features\")\n",
        "    print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
        "    print()\n",
        "\n",
        "    # Your best parameters from previous tuning\n",
        "    best_params = {\n",
        "        'class_weight': None,\n",
        "        'criterion': 'entropy',\n",
        "        'max_depth': 5,\n",
        "        'min_samples_leaf': 5,\n",
        "        'min_samples_split': 2\n",
        "    }\n",
        "\n",
        "    # Store results\n",
        "    results = {}\n",
        "\n",
        "    # 1. Baseline with best parameters on all features\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=ERP_ID, stratify=y)\n",
        "    dt_baseline = DecisionTreeClassifier(**best_params, random_state=ERP_ID)\n",
        "    dt_baseline.fit(X_train, y_train)\n",
        "    baseline_pred = dt_baseline.predict_proba(X_val)[:, 1]\n",
        "    baseline_auc = roc_auc_score(y_val, baseline_pred)\n",
        "    results['Baseline (Best Params)'] = baseline_auc\n",
        "\n",
        "    print(f\"Baseline with best parameters: {baseline_auc:.4f}\")\n",
        "\n",
        "    # 2. Optimized backward elimination\n",
        "    selected_features, backward_auc, removed_features = backward_elimination_optimized(X, y, best_params)\n",
        "    results['After Backward Elimination'] = backward_auc\n",
        "\n",
        "    # 3. Final model\n",
        "    final_model, final_auc = final_tuned_model(X, y, selected_features, best_params)\n",
        "    results['Final Optimized Model'] = final_auc\n",
        "\n",
        "    # 4. Feature importance analysis\n",
        "    feature_imp_df = feature_importance_analysis(final_model, selected_features)\n",
        "\n",
        "    # Final comprehensive report\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"FINAL COMPREHENSIVE RESULTS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    print(f\"\\nPERFORMANCE SUMMARY:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'Stage':35} | {'AUROC':8} | {'Improvement':12} | {'Features':10}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    baseline = results['Baseline (Best Params)']\n",
        "    for stage, auroc in results.items():\n",
        "        improvement = auroc - baseline\n",
        "        if stage == 'Baseline (Best Params)':\n",
        "            feature_count = X.shape[1]\n",
        "        elif stage == 'After Backward Elimination':\n",
        "            feature_count = len(selected_features)\n",
        "        else:\n",
        "            feature_count = len(selected_features)\n",
        "\n",
        "        print(f\"{stage:35} | {auroc:.4f}  | {improvement:+.4f} | {feature_count:10}\")\n",
        "\n",
        "    total_improvement = final_auc - baseline_auc\n",
        "    print(f\"\\nTOTAL IMPROVEMENT: +{total_improvement:.4f}\")\n",
        "    print(f\"RELATIVE IMPROVEMENT: {(total_improvement/baseline_auc)*100:.1f}%\")\n",
        "\n",
        "    print(f\"\\nKEY ACHIEVEMENTS:\")\n",
        "    print(\"â€¢ Combined backward elimination with optimized hyperparameters\")\n",
        "    print(\"â€¢ Reduced feature set while maintaining performance\")\n",
        "    print(\"â€¢ Achieved best possible AUROC through feature optimization\")\n",
        "\n",
        "    # Save all results\n",
        "    performance_df = pd.DataFrame([\n",
        "        {'Stage': 'Baseline (Best Params)', 'AUROC': baseline_auc, 'Improvement': 0.0000, 'Features': X.shape[1]},\n",
        "        {'Stage': 'After Backward Elimination', 'AUROC': backward_auc, 'Improvement': backward_auc - baseline_auc, 'Features': len(selected_features)},\n",
        "        {'Stage': 'Final Optimized Model', 'AUROC': final_auc, 'Improvement': final_auc - baseline_auc, 'Features': len(selected_features)}\n",
        "    ])\n",
        "\n",
        "    performance_df.to_csv('final_optimized_results.csv', index=False)\n",
        "    feature_imp_df.to_csv('final_optimized_feature_importance.csv', index=False)\n",
        "\n",
        "    print(f\"\\nRESULTS SAVED:\")\n",
        "    print(\"- 'final_optimized_results.csv'\")\n",
        "    print(\"- 'final_optimized_feature_importance.csv'\")\n",
        "    print(\"- 'final_feature_analysis.png'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0cGd8sHEmKi"
      },
      "source": [
        "Colab is a virtual machine you can access directly. To run commands at the VM's terminal, prefix the line with an exclamation point (!).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuWRpQdatAIU"
      },
      "source": [
        "**Exercise**\n",
        "\n",
        "Create a code cell underneath this text cell and add code to:\n",
        "\n",
        "\n",
        "*   List the path of the current directory (pwd)\n",
        "* Go to / (cd) and list the content (ls -l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b5jv0ouFREV"
      },
      "source": [
        "All usage of Colab in this course is completely free or charge. Even GPU usage is provided free of charge for some hours of usage every day.\n",
        "\n",
        "**Using GPUs**\n",
        "* Many of the exercises in the course executes more quickly by using GPU runtime: Runtime | Change runtime type | Hardware accelerator | GPU\n",
        "\n",
        "**Some final words on Colab**\n",
        "*   You execute each cell in order, you can edit & re-execute cells if you want\n",
        "*   Sometimes, this could have unintended consequences. For example, if you add a dimension to an array and execute the cell multiple times, then the cells after may not work. If you encounter problem reset your environment:\n",
        "  *   Runtime -> Restart runtime... Resets your Python shell\n",
        "  *   Runtime -> Restart all runtimes... Will reset the Colab image, and get you back to a 100% clean environment\n",
        "* You can also clear the output in the Colab by doing: Edit -> Clear all outputs\n",
        "* Colabs in this course are loaded from GitHub. Save to your Google Drive if you want a copy with your code/output: File -> Save a copy in Drive...\n",
        "\n",
        "**Learn More**\n",
        "*   Check out [this](https://www.youtube.com/watch?v=inN8seMm7UI&list=PLQY2H8rRoyvwLbzbnKJ59NkZvQAW9wLbx&index=3) episode of #CodingTensorFlow, and don't forget to subscribe to the YouTube channel ;)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "l01c01_introduction_to_colab_and_python.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}